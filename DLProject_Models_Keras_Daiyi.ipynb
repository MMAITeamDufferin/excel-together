{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM \n",
    "from keras.layers import GRU\n",
    "from keras.layers import Bidirectional,MaxPooling1D,Flatten,TimeDistributed\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from keras.utils.np_utils import to_categorical \n",
    "\n",
    "from scipy.ndimage import zoom\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract http://archive.ics.uci.edu/ml/machine-learning-databases/00447/ to same directory as this script\n",
    "\n",
    "# List files\n",
    "one = ['.\\\\data\\\\TS1.txt', '.\\\\data\\\\TS2.txt', '.\\\\data\\\\TS3.txt', '.\\\\data\\\\TS4.txt', '.\\\\data\\\\VS1.txt', '.\\\\data\\\\CE.txt', '.\\\\data\\\\CP.txt', '.\\\\data\\\\SE.txt']\n",
    "ten = ['.\\\\data\\\\FS1.txt', '.\\\\data\\\\FS2.txt']\n",
    "hundred = ['.\\\\data\\\\PS1.txt', '.\\\\data\\\\PS2.txt', '.\\\\data\\\\PS3.txt', '.\\\\data\\\\PS4.txt', '.\\\\data\\\\PS5.txt', '.\\\\data\\\\PS6.txt', '.\\\\data\\\\EPS1.txt']\n",
    "\n",
    "# Parse condition profiles\n",
    "df_profile = pd.read_table('.\\\\data\\\\profile.txt', header=None)\n",
    "df_profile = df_profile.values.reshape(2205, 1, 5)\n",
    "df_profile = zoom(df_profile, (1,6000,1))\n",
    "\n",
    "# Parse 1 Hz measurements\n",
    "df_one =  np.stack([pd.read_table(x, header=None) for x in one], axis=2)\n",
    "df_one = zoom(df_one, (1, 100, 1))\n",
    "\n",
    "# Parse 10 Hz measurements\n",
    "df_ten =  np.stack([pd.read_table(x, header=None) for x in ten], axis=2)\n",
    "df_ten = zoom(df_ten, (1, 10, 1))\n",
    "\n",
    "# Parse 100 Hz measurements\n",
    "df_hundred = np.stack([pd.read_table(x, header=None) for x in hundred], axis=2)\n",
    "\n",
    "# Concatenate all data\n",
    "df = np.concatenate([df_profile, df_one, df_ten, df_hundred], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "val = 0.2\n",
    "test = 0.1\n",
    "train = 1 - val - test\n",
    "\n",
    "X_train = df[:int(train*df.shape[0])+1:,::,[i not in [1] for i in range(df.shape[2])]]\n",
    "X_val = df[int(train*df.shape[0])+1:int(train*df.shape[0])+int(val*df.shape[0])+1:,::,[i not in [1] for i in range(df.shape[2])]]\n",
    "X_test = df[int(train*df.shape[0])+int(val*df.shape[0])+1::,::,[i not in [1] for i in range(df.shape[2])]]\n",
    "\n",
    "oh_target = (np.arange(df[:,0,1].max()+1) == df[:,0,1][...,None]).astype(int)\n",
    "oh_target = np.delete(oh_target,np.where(~oh_target.any(axis=0))[0], axis=1)\n",
    "\n",
    "y_train = oh_target[:int(train*oh_target.shape[0])+1:,]\n",
    "y_val = oh_target[int(train*oh_target.shape[0])+1:int(train*oh_target.shape[0])+int(val*oh_target.shape[0])+1:,]\n",
    "y_test = oh_target[int(train*oh_target.shape[0])+int(val*oh_target.shape[0])+1::,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1544, 6000, 21),\n",
       " (441, 6000, 21),\n",
       " (220, 6000, 21),\n",
       " (1544, 4),\n",
       " (441, 4),\n",
       " (220, 4))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify shape for each dataset\n",
    "X_train.shape,X_val.shape,X_test.shape,y_train.shape,y_val.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the first model\n",
    "# LSTM RNN\n",
    "model_1 = Sequential()\n",
    "model_1.add(LSTM(4))\n",
    "model_1.add(Dense(y_train.shape[1]))\n",
    "model_1.add(Activation('softmax'))\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 31s - loss: 1.2951 - acc: 0.5337\n",
      "Epoch 2/10\n",
      " - 21s - loss: 1.2943 - acc: 0.5337\n",
      "Epoch 3/10\n",
      " - 21s - loss: 1.2935 - acc: 0.5337\n",
      "Epoch 4/10\n",
      " - 18s - loss: 1.2927 - acc: 0.5337\n",
      "Epoch 5/10\n",
      " - 19s - loss: 1.2920 - acc: 0.5337\n",
      "Epoch 6/10\n",
      " - 19s - loss: 1.2912 - acc: 0.5337\n",
      "Epoch 7/10\n",
      " - 20s - loss: 1.2904 - acc: 0.5337\n",
      "Epoch 8/10\n",
      " - 19s - loss: 1.2896 - acc: 0.5337\n",
      "Epoch 9/10\n",
      " - 19s - loss: 1.2888 - acc: 0.5337\n",
      "Epoch 10/10\n",
      " - 18s - loss: 1.2880 - acc: 0.5337\n"
     ]
    }
   ],
   "source": [
    "#fit model 1\n",
    "history_1 = model_1.fit(X_train, y_train, epochs=10, batch_size=len(X_train), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441/441 [==============================] - 5s 12ms/step\n",
      "test loss:  1.2946663827312237\n",
      "test accuracy:  0.5170068025859305\n"
     ]
    }
   ],
   "source": [
    "#evaluation for model_1 validation dataset\n",
    "score_model1=model_1.evaluate(x=X_val, y=y_val) \n",
    "# calcute accuracy and loss \n",
    "print('test loss: ', score_model1[0])\n",
    "print('test accuracy: ', score_model1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the 2nd model\n",
    "# GRU\n",
    "model_2 = Sequential()\n",
    "model_2.add(GRU(4))\n",
    "model_2.add(Dense(y_train.shape[1]))\n",
    "model_2.add(Activation('softmax'))\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 15s - loss: 1.3863 - acc: 0.1554\n",
      "Epoch 2/10\n",
      " - 15s - loss: 1.3857 - acc: 0.5337\n",
      "Epoch 3/10\n",
      " - 15s - loss: 1.3852 - acc: 0.5337\n",
      "Epoch 4/10\n",
      " - 16s - loss: 1.3847 - acc: 0.5337\n",
      "Epoch 5/10\n",
      " - 16s - loss: 1.3842 - acc: 0.5337\n",
      "Epoch 6/10\n",
      " - 17s - loss: 1.3836 - acc: 0.5337\n",
      "Epoch 7/10\n",
      " - 18s - loss: 1.3830 - acc: 0.5337\n",
      "Epoch 8/10\n",
      " - 17s - loss: 1.3825 - acc: 0.5337\n",
      "Epoch 9/10\n",
      " - 16s - loss: 1.3819 - acc: 0.5337\n",
      "Epoch 10/10\n",
      " - 18s - loss: 1.3813 - acc: 0.5337\n"
     ]
    }
   ],
   "source": [
    "#fit model 2\n",
    "history_2 = model_2.fit(X_train, y_train, epochs=10, batch_size=len(X_train), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441/441 [==============================] - 5s 10ms/step\n",
      "test loss:  1.3811186292544515\n",
      "test accuracy:  0.5170068025859305\n"
     ]
    }
   ],
   "source": [
    "#evaluation for model_2 validation dataset\n",
    "score_model2=model_2.evaluate(x=X_val, y=y_val) \n",
    "# calcute accuracy and loss \n",
    "print('test loss: ', score_model2[0])\n",
    "print('test accuracy: ', score_model2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the 3rd model\n",
    "# BiLSTM + CNN\n",
    "\n",
    "model_3 = Sequential() \n",
    "model_3.add(Bidirectional(LSTM(4, return_sequences=True)))\n",
    "model_3.add(TimeDistributed(Dense(4)))\n",
    "model_3.add(Activation('softplus'))\n",
    "model_3.add(MaxPooling1D(5))\n",
    "model_3.add(Flatten())\n",
    "model_3.add(Dense(y_train.shape[1]))\n",
    "model_3.add(Activation('softmax'))\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 221s - loss: 2.0441 - acc: 0.1554\n",
      "Epoch 2/10\n",
      " - 272s - loss: 1.6295 - acc: 0.5337\n",
      "Epoch 3/10\n",
      " - 200s - loss: 1.7415 - acc: 0.5337\n",
      "Epoch 4/10\n",
      " - 193s - loss: 1.9198 - acc: 0.1554\n",
      "Epoch 5/10\n",
      " - 183s - loss: 1.3111 - acc: 0.5337\n",
      "Epoch 6/10\n",
      " - 170s - loss: 1.3548 - acc: 0.5337\n",
      "Epoch 7/10\n",
      " - 157s - loss: 1.4960 - acc: 0.5337\n",
      "Epoch 8/10\n",
      " - 121s - loss: 1.5506 - acc: 0.5337\n",
      "Epoch 9/10\n",
      " - 151s - loss: 1.5491 - acc: 0.5337\n",
      "Epoch 10/10\n",
      " - 168s - loss: 1.4701 - acc: 0.5337\n"
     ]
    }
   ],
   "source": [
    "#fit model 3\n",
    "history_3 = model_3.fit(X_train, y_train, epochs=10, batch_size=len(X_train), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441/441 [==============================] - 9s 20ms/step\n",
      "test loss:  1.3168306929183926\n",
      "test accuracy:  0.5170068025859305\n"
     ]
    }
   ],
   "source": [
    "#evaluation for model_3 validation dataset\n",
    "score_model3=model_3.evaluate(x=X_val, y=y_val) \n",
    "# calcute accuracy and loss \n",
    "print('test loss: ', score_model3[0])\n",
    "print('test accuracy: ', score_model3[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
