{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract http://archive.ics.uci.edu/ml/machine-learning-databases/00447/ to same directory as this script\n",
    "\n",
    "\n",
    "\n",
    "# List files\n",
    "\n",
    "one = ['TS1.txt', 'TS2.txt', 'TS3.txt', 'TS4.txt', 'VS1.txt', 'CE.txt', 'CP.txt', 'SE.txt']\n",
    "\n",
    "ten = ['FS1.txt', 'FS2.txt']\n",
    "\n",
    "hundred = ['PS1.txt', 'PS2.txt', 'PS3.txt', 'PS4.txt', 'PS5.txt', 'PS6.txt', 'EPS1.txt']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse condition profiles\n",
    "\n",
    "df_profile = pd.read_table('profile.txt', header=None)\n",
    "\n",
    "df_profile = df_profile.values.reshape(2205, 1, 5)\n",
    "\n",
    "df_profile = zoom(df_profile, (1,6000,1))\n",
    "\n",
    "\n",
    "\n",
    "# Parse 1 Hz measurements\n",
    "\n",
    "df_one =  np.stack([pd.read_table(x, header=None) for x in one], axis=2)\n",
    "\n",
    "df_one = zoom(df_one, (1, 100, 1))\n",
    "\n",
    "\n",
    "\n",
    "# Parse 10 Hz measurements\n",
    "\n",
    "df_ten =  np.stack([pd.read_table(x, header=None) for x in ten], axis=2)\n",
    "\n",
    "df_ten = zoom(df_ten, (1, 10, 1))\n",
    "\n",
    "\n",
    "\n",
    "# Parse 100 Hz measurements\n",
    "\n",
    "df_hundred = np.stack([pd.read_table(x, header=None) for x in hundred], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all data\n",
    "\n",
    "df = np.concatenate([df_profile, df_one, df_ten, df_hundred], axis=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "\n",
    "val = 0.2\n",
    "\n",
    "test = 0.1\n",
    "\n",
    "train = 1 - val - test\n",
    "\n",
    "\n",
    "\n",
    "X_train = df[:int(train*df.shape[0])+1:,::,[i not in [1] for i in range(df.shape[2])]]\n",
    "\n",
    "X_val = df[int(train*df.shape[0])+1:int(train*df.shape[0])+int(val*df.shape[0])+1:,::,[i not in [1] for i in range(df.shape[2])]]\n",
    "\n",
    "X_test = df[int(train*df.shape[0])+int(val*df.shape[0])+1::,::,[i not in [1] for i in range(df.shape[2])]]\n",
    "\n",
    "\n",
    "\n",
    "oh_target = (np.arange(df[:,0,1].max()+1) == df[:,0,1][...,None]).astype(int)\n",
    "\n",
    "oh_target = np.delete(oh_target,np.where(~oh_target.any(axis=0))[0], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "y_train = oh_target[:int(train*oh_target.shape[0])+1:,]\n",
    "\n",
    "y_val = oh_target[int(train*oh_target.shape[0])+1:int(train*oh_target.shape[0])+int(val*oh_target.shape[0])+1:,]\n",
    "\n",
    "y_test = oh_target[int(train*oh_target.shape[0])+int(val*oh_target.shape[0])+1::,]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 Iteration: 5 Train loss: 1.357857 Train acc: 0.360000\n",
      "Epoch: 1/1 Iteration: 10 Train loss: 1.336112 Train acc: 0.360000\n",
      "Epoch: 1/1 Iteration: 15 Train loss: 1.321885 Train acc: 0.540000\n",
      "Epoch: 1/1 Iteration: 20 Train loss: 1.424419 Train acc: 0.220000\n",
      "Epoch: 1/1 Iteration: 25 Train loss: 1.365082 Train acc: 0.360000\n",
      "Epoch: 1/1 Iteration: 25 Validation loss: 1.247948 Validation acc: 0.560000\n"
     ]
    }
   ],
   "source": [
    "def sample_batch(X, y, batch_size):\n",
    "\n",
    "    for b in range(0, len(X)-(len(X)%batch_size)-batch_size, batch_size):\n",
    "\n",
    "        yield X[b:b + batch_size], y[b:b + batch_size]\n",
    "\n",
    "\n",
    "\n",
    "# Constants\n",
    "\n",
    "samples, seq_len, features = X_train.shape\n",
    "\n",
    "n_classes = y_train.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "lstm_size = 3*features\n",
    "\n",
    "lstm_layers = 2\n",
    "\n",
    "dropout = 0.8\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "learning_rate = 0.0001  # default is 0.001\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    inputs = tf.placeholder(dtype=tf.float32, shape=[None, seq_len, features], name='inputs')\n",
    "\n",
    "    with tf.name_scope(\"Target\"):\n",
    "\n",
    "        target = tf.placeholder(dtype=tf.int32, shape=[None, n_classes], name='target')\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep')\n",
    "\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    lstm_in = tf.transpose(inputs, [1, 0, 2])  # reshape into (seq_len, samples, features)\n",
    "\n",
    "    lstm_in = tf.reshape(lstm_in, [-1, features])  # Now (seq_len*samples, features)\n",
    "\n",
    "\n",
    "\n",
    "    # To cells\n",
    "\n",
    "    lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=None)\n",
    "\n",
    "\n",
    "\n",
    "    # Open up the tensor into a list of seq_len pieces\n",
    "\n",
    "    lstm_in = tf.split(lstm_in, seq_len, 0)\n",
    "\n",
    "\n",
    "\n",
    "    # Add LSTM layers\n",
    "\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32, initial_state=initial_state)\n",
    "\n",
    "\n",
    "\n",
    "    # We only need the last output tensor to pass into a classifier\n",
    "\n",
    "    logits = tf.layers.dense(outputs[-1], n_classes, name='logits')\n",
    "\n",
    "\n",
    "\n",
    "    # Cost function and optimizer\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=target))\n",
    "\n",
    "\n",
    "\n",
    "    # No grad clipping\n",
    "\n",
    "    # optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "    # Grad clipping\n",
    "\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "    gradients = train_op.compute_gradients(cost)\n",
    "\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "\n",
    "    optimizer = train_op.apply_gradients(capped_gradients)\n",
    "\n",
    "\n",
    "\n",
    "    # Accuracy\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(target, 1))\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "\n",
    "\n",
    "if (os.path.exists('checkpoints') == False):\n",
    "\n",
    "    os.system('mkdir checkpoints')\n",
    "\n",
    "\n",
    "\n",
    "train_acc = []\n",
    "\n",
    "train_loss = []\n",
    "\n",
    "\n",
    "\n",
    "validation_acc = []\n",
    "\n",
    "validation_loss = []\n",
    "\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    iteration = 1\n",
    "\n",
    "\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # Initialize\n",
    "\n",
    "        state = sess.run(initial_state)\n",
    "\n",
    "\n",
    "\n",
    "        # Loop over batches\n",
    "\n",
    "        for x, y in sample_batch(X_train, y_train, batch_size):\n",
    "\n",
    "\n",
    "\n",
    "            # Feed dictionary\n",
    "\n",
    "            feed = {inputs: x, target: y, keep_prob: dropout, initial_state: state}\n",
    "\n",
    "\n",
    "\n",
    "            loss, _, state, acc = sess.run([cost, optimizer, final_state, accuracy],\n",
    "\n",
    "                                           feed_dict=feed)\n",
    "\n",
    "            train_acc.append(acc)\n",
    "\n",
    "            train_loss.append(loss)\n",
    "\n",
    "\n",
    "\n",
    "            # Print at each 5 iters\n",
    "\n",
    "            if (iteration % 5 == 0):\n",
    "\n",
    "                print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "\n",
    "                      \"Train loss: {:6f}\".format(loss),\n",
    "\n",
    "                      \"Train acc: {:.6f}\".format(acc))\n",
    "\n",
    "\n",
    "\n",
    "            # Compute validation loss at every 25 iterations\n",
    "\n",
    "            if (iteration % 25 == 0):\n",
    "\n",
    "\n",
    "\n",
    "                # Initiate for validation set\n",
    "\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "                val_acc_ = []\n",
    "\n",
    "                val_loss_ = []\n",
    "\n",
    "                for x_v, y_v in sample_batch(X_val, y_val, batch_size):\n",
    "\n",
    "                    # Feed\n",
    "\n",
    "                    feed = {inputs: x_v, target: y_v, keep_prob: 1.0, initial_state: val_state}\n",
    "\n",
    "\n",
    "\n",
    "                    # Loss\n",
    "\n",
    "                    loss_v, state_v, acc_v = sess.run([cost, final_state, accuracy], feed_dict=feed)\n",
    "\n",
    "\n",
    "\n",
    "                    val_acc_.append(acc_v)\n",
    "\n",
    "                    val_loss_.append(loss_v)\n",
    "\n",
    "\n",
    "\n",
    "                # Print info\n",
    "\n",
    "                print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "\n",
    "                      \"Validation loss: {:6f}\".format(np.mean(val_loss_)),\n",
    "\n",
    "                      \"Validation acc: {:.6f}\".format(np.mean(val_acc_)))\n",
    "\n",
    "\n",
    "\n",
    "                # Store\n",
    "\n",
    "                validation_acc.append(np.mean(val_acc_))\n",
    "\n",
    "                validation_loss.append(np.mean(val_loss_))\n",
    "\n",
    "\n",
    "\n",
    "            # Iterate\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "\n",
    "\n",
    "    saver.save(sess, \"checkpoints/lstm.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3 Iteration: 5 Train loss: 1.406535 Train acc: 0.240000\n",
      "Epoch: 1/3 Iteration: 10 Train loss: 1.343435 Train acc: 0.030000\n",
      "Epoch: 1/3 Iteration: 15 Train loss: 1.405002 Train acc: 0.230000\n",
      "Epoch: 2/3 Iteration: 20 Train loss: 1.408165 Train acc: 0.220000\n",
      "Epoch: 2/3 Iteration: 25 Train loss: 1.291842 Train acc: 0.510000\n",
      "Epoch: 2/3 Iteration: 25 Validation loss: 1.299136 Validation acc: 0.600000\n",
      "Epoch: 3/3 Iteration: 30 Train loss: 1.146003 Train acc: 0.830000\n",
      "Epoch: 3/3 Iteration: 35 Train loss: 1.411832 Train acc: 0.290000\n",
      "Epoch: 3/3 Iteration: 40 Train loss: 1.402940 Train acc: 0.320000\n"
     ]
    }
   ],
   "source": [
    "def sample_batch(X, y, batch_size):\n",
    "\n",
    "    for b in range(0, len(X)-(len(X)%batch_size)-batch_size, batch_size):\n",
    "\n",
    "        yield X[b:b + batch_size], y[b:b + batch_size]\n",
    "\n",
    "\n",
    "\n",
    "# Constants\n",
    "\n",
    "samples, seq_len, features = X_train.shape\n",
    "\n",
    "n_classes = y_train.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "lstm_size = 3*features\n",
    "\n",
    "lstm_layers = 2\n",
    "\n",
    "dropout = 0.8\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "learning_rate = 0.0001  # default is 0.001\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    inputs = tf.placeholder(dtype=tf.float32, shape=[None, seq_len, features], name='inputs')\n",
    "\n",
    "    with tf.name_scope(\"Target\"):\n",
    "\n",
    "        target = tf.placeholder(dtype=tf.int32, shape=[None, n_classes], name='target')\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep')\n",
    "\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    lstm_in = tf.transpose(inputs, [1, 0, 2])  # reshape into (seq_len, samples, features)\n",
    "\n",
    "    lstm_in = tf.reshape(lstm_in, [-1, features])  # Now (seq_len*samples, features)\n",
    "\n",
    "\n",
    "\n",
    "    # To cells\n",
    "\n",
    "    lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=None)\n",
    "\n",
    "\n",
    "\n",
    "    # Open up the tensor into a list of seq_len pieces\n",
    "\n",
    "    lstm_in = tf.split(lstm_in, seq_len, 0)\n",
    "\n",
    "\n",
    "\n",
    "    # Add LSTM layers\n",
    "\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32, initial_state=initial_state)\n",
    "\n",
    "\n",
    "\n",
    "    # We only need the last output tensor to pass into a classifier\n",
    "\n",
    "    logits = tf.layers.dense(outputs[-1], n_classes, name='logits')\n",
    "\n",
    "\n",
    "\n",
    "    # Cost function and optimizer\n",
    "\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=target))\n",
    "\n",
    "\n",
    "\n",
    "    # No grad clipping\n",
    "\n",
    "    # optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "    # Grad clipping\n",
    "\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "    gradients = train_op.compute_gradients(cost)\n",
    "\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "\n",
    "    optimizer = train_op.apply_gradients(capped_gradients)\n",
    "\n",
    "\n",
    "\n",
    "    # Accuracy\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(target, 1))\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "\n",
    "\n",
    "if (os.path.exists('checkpoints') == False):\n",
    "\n",
    "    os.system('mkdir checkpoints')\n",
    "\n",
    "\n",
    "\n",
    "train_acc = []\n",
    "\n",
    "train_loss = []\n",
    "\n",
    "\n",
    "\n",
    "validation_acc = []\n",
    "\n",
    "validation_loss = []\n",
    "\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    iteration = 2\n",
    "\n",
    "\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # Initialize\n",
    "\n",
    "        state = sess.run(initial_state)\n",
    "\n",
    "\n",
    "\n",
    "        # Loop over batches\n",
    "\n",
    "        for x, y in sample_batch(X_train, y_train, batch_size):\n",
    "\n",
    "\n",
    "\n",
    "            # Feed dictionary\n",
    "\n",
    "            feed = {inputs: x, target: y, keep_prob: dropout, initial_state: state}\n",
    "\n",
    "\n",
    "\n",
    "            loss, _, state, acc = sess.run([cost, optimizer, final_state, accuracy],\n",
    "\n",
    "                                           feed_dict=feed)\n",
    "\n",
    "            train_acc.append(acc)\n",
    "\n",
    "            train_loss.append(loss)\n",
    "\n",
    "\n",
    "\n",
    "            # Print at each 5 iters\n",
    "\n",
    "            if (iteration % 5 == 0):\n",
    "\n",
    "                print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "\n",
    "                      \"Train loss: {:6f}\".format(loss),\n",
    "\n",
    "                      \"Train acc: {:.6f}\".format(acc))\n",
    "\n",
    "\n",
    "\n",
    "            # Compute validation loss at every 25 iterations\n",
    "\n",
    "            if (iteration % 25 == 0):\n",
    "\n",
    "\n",
    "\n",
    "                # Initiate for validation set\n",
    "\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "                val_acc_ = []\n",
    "\n",
    "                val_loss_ = []\n",
    "\n",
    "                for x_v, y_v in sample_batch(X_val, y_val, batch_size):\n",
    "\n",
    "                    # Feed\n",
    "\n",
    "                    feed = {inputs: x_v, target: y_v, keep_prob: 1.0, initial_state: val_state}\n",
    "\n",
    "\n",
    "\n",
    "                    # Loss\n",
    "\n",
    "                    loss_v, state_v, acc_v = sess.run([cost, final_state, accuracy], feed_dict=feed)\n",
    "\n",
    "\n",
    "\n",
    "                    val_acc_.append(acc_v)\n",
    "\n",
    "                    val_loss_.append(loss_v)\n",
    "\n",
    "\n",
    "\n",
    "                # Print info\n",
    "\n",
    "                print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "\n",
    "                      \"Validation loss: {:6f}\".format(np.mean(val_loss_)),\n",
    "\n",
    "                      \"Validation acc: {:.6f}\".format(np.mean(val_acc_)))\n",
    "\n",
    "\n",
    "\n",
    "                # Store\n",
    "\n",
    "                validation_acc.append(np.mean(val_acc_))\n",
    "\n",
    "                validation_loss.append(np.mean(val_loss_))\n",
    "\n",
    "\n",
    "\n",
    "            # Iterate\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "\n",
    "\n",
    "    saver.save(sess, \"checkpoints/lstm.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
